{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Bird Classification using ResNet34"]},{"cell_type":"markdown","metadata":{},"source":["## Importing the Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T14:04:36.473269Z","iopub.status.busy":"2023-01-18T14:04:36.472756Z","iopub.status.idle":"2023-01-18T14:04:36.482978Z","shell.execute_reply":"2023-01-18T14:04:36.481718Z","shell.execute_reply.started":"2023-01-18T14:04:36.473225Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import transforms as T\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import os\n","\n","BASE_DIR = \"./data/\"\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Device:\", device)"]},{"cell_type":"markdown","metadata":{},"source":["Setting seed for reproducibility"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T14:04:36.486482Z","iopub.status.busy":"2023-01-18T14:04:36.485714Z","iopub.status.idle":"2023-01-18T14:04:36.492961Z","shell.execute_reply":"2023-01-18T14:04:36.491874Z","shell.execute_reply.started":"2023-01-18T14:04:36.486442Z"},"trusted":true},"outputs":[],"source":["seed = 42\n","def set_seed(seed):\n","    torch.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    np.random.seed(seed)\n","\n","set_seed(seed)"]},{"cell_type":"markdown","metadata":{},"source":["## Loading the Dataset"]},{"cell_type":"markdown","metadata":{},"source":["The dataset is taken from [Kaggle](https://www.kaggle.com/datasets/gpiosenka/100-bird-species) and consists of 70,658 training, 2250 test and 2259 validation images of birds belonging to 450 different species. The images are of size 224x224x3(HxWxC). The `birds.csv` files consists of the class ID, file paths, labels, scientific label and the dataset[train, test, val] to which the image belongs to. The images are read using python `PIL` library and transforms are applied."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T14:04:36.495763Z","iopub.status.busy":"2023-01-18T14:04:36.494984Z","iopub.status.idle":"2023-01-18T14:04:36.922346Z","shell.execute_reply":"2023-01-18T14:04:36.921376Z","shell.execute_reply.started":"2023-01-18T14:04:36.495724Z"},"trusted":true},"outputs":[],"source":["paths_df = pd.read_csv(os.path.join(BASE_DIR, \"birds.csv\"))\n","paths_df.drop(46620, axis= 0, inplace=True) # The dataset as of 9-Jan-2023 contains a file that is \n","# present at the given index whose dimension is not 224x224. Removing the file to avoid unnecessary complexity in the code\n","labels = paths_df[\"class id\"].unique()\n","bird_name_map = {int(i): paths_df[paths_df[\"class id\"] == i][\"labels\"].values[0] for i in labels}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T14:04:36.925610Z","iopub.status.busy":"2023-01-18T14:04:36.925039Z","iopub.status.idle":"2023-01-18T14:04:36.944752Z","shell.execute_reply":"2023-01-18T14:04:36.943733Z","shell.execute_reply.started":"2023-01-18T14:04:36.925570Z"},"trusted":true},"outputs":[],"source":["paths_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T14:04:36.946811Z","iopub.status.busy":"2023-01-18T14:04:36.946000Z","iopub.status.idle":"2023-01-18T14:04:36.975317Z","shell.execute_reply":"2023-01-18T14:04:36.974102Z","shell.execute_reply.started":"2023-01-18T14:04:36.946773Z"},"trusted":true},"outputs":[],"source":["paths_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T14:04:36.977929Z","iopub.status.busy":"2023-01-18T14:04:36.977013Z","iopub.status.idle":"2023-01-18T14:04:37.189679Z","shell.execute_reply":"2023-01-18T14:04:37.188746Z","shell.execute_reply.started":"2023-01-18T14:04:36.977892Z"},"trusted":true},"outputs":[],"source":["dataset_split = paths_df[\"data set\"].value_counts()\n","plt.bar(dataset_split.index, dataset_split.values)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T14:04:37.192753Z","iopub.status.busy":"2023-01-18T14:04:37.192153Z","iopub.status.idle":"2023-01-18T14:04:37.200444Z","shell.execute_reply":"2023-01-18T14:04:37.199389Z","shell.execute_reply.started":"2023-01-18T14:04:37.192716Z"},"trusted":true},"outputs":[],"source":["class BirdDataset(torch.utils.data.Dataset):\n","    def __init__(self, df, transform=None):\n","        self.df = df\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.df.iloc[idx]['filepaths']\n","        img_path = os.path.join(BASE_DIR, img_path)\n","        img = Image.open(img_path)\n","\n","        if self.transform:\n","            img = self.transform(img)\n","        label = self.df.iloc[idx]['class id']\n","        \n","        label = torch.tensor(label, dtype=torch.long)\n","        return img, label"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T14:04:37.203377Z","iopub.status.busy":"2023-01-18T14:04:37.201854Z","iopub.status.idle":"2023-01-18T14:04:37.238541Z","shell.execute_reply":"2023-01-18T14:04:37.237560Z","shell.execute_reply.started":"2023-01-18T14:04:37.203341Z"},"trusted":true},"outputs":[],"source":["normalise_means = [0.4914, 0.4822, 0.4465]\n","normalise_stds = [0.2023, 0.1994, 0.2010]\n","\n","train_transform = T.Compose([T.RandomHorizontalFlip(),\n","    T.ToTensor(),\n","    T.Normalize(normalise_means, normalise_stds),])\n","\n","test_transforms = T.Compose([T.ToTensor(), T.Normalize(normalise_means, normalise_stds)])\n","    \n","train_df = paths_df[paths_df['data set'] == 'train']\n","test_df = paths_df[paths_df['data set'] == 'test']\n","val_df = paths_df[paths_df['data set'] == 'valid']\n","\n","train_dataset = BirdDataset(train_df, train_transform)\n","test_dataset = BirdDataset(test_df, test_transforms)\n","val_dataset = BirdDataset(val_df, test_transforms)\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=True, num_workers=2)\n","val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=True, num_workers=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T14:04:37.240427Z","iopub.status.busy":"2023-01-18T14:04:37.240038Z","iopub.status.idle":"2023-01-18T14:04:39.993957Z","shell.execute_reply":"2023-01-18T14:04:39.992487Z","shell.execute_reply.started":"2023-01-18T14:04:37.240391Z"},"trusted":true},"outputs":[],"source":["def denormalize(img):\n","    means = torch.tensor(normalise_means).view(3, 1, 1)\n","    stds = torch.tensor(normalise_stds).view(3, 1, 1)\n","    return img * stds + means\n","\n","def show_batch(images, labels, class_map):\n","    fig, ax = plt.subplots(4, 4, figsize=(15, 15))\n","    fig.tight_layout()\n","    for i in range(4):\n","        for j in range(4):\n","            image = denormalize(images[i*4 + j]).permute(1, 2, 0)\n","            label = labels[i*4 + j].item()\n","            ax[i][j].imshow((image.numpy() * 255).astype(np.uint8))\n","            title = class_map[label]\n","            ax[i][j].set_title(title)\n","            ax[i][j].axis('off')\n","            ax[i][j].title.set_fontsize(10)\n","\n","    plt.show()\n","\n","images, labels = next(iter(train_loader))\n","show_batch(images, labels, bird_name_map)"]},{"cell_type":"markdown","metadata":{},"source":["## Defining the Model"]},{"cell_type":"markdown","metadata":{},"source":["The project uses a ResNet34 model that is implemented according to the [paper](https://arxiv.org/abs/1512.03385). The only difference is that the model implemented here uses log softmax instead of softmax in the output layer as log softmax gives better accuracy and faster convergence rate.  "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T14:04:39.998038Z","iopub.status.busy":"2023-01-18T14:04:39.997174Z","iopub.status.idle":"2023-01-18T14:04:40.031919Z","shell.execute_reply":"2023-01-18T14:04:40.030561Z","shell.execute_reply.started":"2023-01-18T14:04:39.997999Z"},"trusted":true},"outputs":[],"source":["class BasicBlock(nn.Module):\n","\n","    def __init__(self, in_channels, out_channels, stride= 1):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride= stride, padding= 1)\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, stride= 1, padding= 1)\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU(inplace= True)\n","\n","        self.downsample = nn.Sequential()\n","        \n","        if stride != 1:\n","            self.downsample = nn.Sequential(\n","                nn.Conv2d(in_channels, out_channels, 1, stride),\n","                nn.BatchNorm2d(out_channels))\n","\n","    def forward(self, x):\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        out += self.downsample(x)\n","\n","        out = self.relu(out)\n","\n","        return out\n","\n","class ResNet34(nn.Module):\n","    def __init__(self, num_classes) -> None:\n","        super().__init__()\n","\n","        self.conv1 = nn.Sequential(nn.Conv2d(3, 64, 7, stride= 2, padding= 3),\n","            nn.BatchNorm2d(64), nn.MaxPool2d(3, stride= 2, padding= 1), nn.ReLU(inplace= True))\n","\n","        self.layer0 = self._make_layer(64, 64, 3, 1)\n","        self.layer1 = self._make_layer(64, 128, 4, 2)\n","        self.layer2 = self._make_layer(128, 256, 6, 2)\n","        self.layer3 = self._make_layer(256, 512, 3, 2)\n","\n","        self.avg_pool = nn.AvgPool2d(7)\n","        self.fc = nn.Linear(512, num_classes)\n","\n","    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n","        \n","        layers = []\n","        layers.append(BasicBlock(in_channels, out_channels, stride))\n","\n","        for i in range(num_blocks - 1):\n","            layers.append(BasicBlock(out_channels, out_channels, 1))\n","\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = self.conv1(x)\n","        \n","        out = self.layer0(out)\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","\n","        out = self.avg_pool(out)\n","        out = out.view(out.size(0), -1)\n","        out = self.fc(out)\n","\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T14:04:40.033996Z","iopub.status.busy":"2023-01-18T14:04:40.033552Z","iopub.status.idle":"2023-01-18T14:04:40.440389Z","shell.execute_reply":"2023-01-18T14:04:40.439264Z","shell.execute_reply.started":"2023-01-18T14:04:40.033949Z"},"trusted":true},"outputs":[],"source":["model = ResNet34(450).to(device)\n","print(model)"]},{"cell_type":"markdown","metadata":{},"source":["## Training "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T14:04:40.442752Z","iopub.status.busy":"2023-01-18T14:04:40.442078Z","iopub.status.idle":"2023-01-18T14:04:40.457085Z","shell.execute_reply":"2023-01-18T14:04:40.456022Z","shell.execute_reply.started":"2023-01-18T14:04:40.442715Z"},"trusted":true},"outputs":[],"source":["def train(model, epochs, optimizer, criterion, scheduler, device, trainloader, valloader):\n","\n","    train_losses = []\n","    train_acc = []\n","    val_losses = []\n","    val_acc = []\n","        \n","    for i in tqdm(range(epochs)):\n","        running_loss = 0.0\n","        running_correct = 0\n","        total = 0\n","\n","        for images, targets in trainloader:\n","            \n","            images, targets = images.to(device), targets.to(device)\n","\n","            optimizer.zero_grad()\n","            output = model(images)\n","            loss = criterion(output, targets)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","            pred = torch.argmax(output, dim=1)\n","        \n","            running_correct += (pred == targets).sum().item()\n","            total += targets.size(0)\n","        \n","        scheduler.step()\n","\n","        train_losses.append(running_loss / len(trainloader))\n","        train_acc.append(running_correct / total)\n","\n","        running_val_loss = 0.0\n","        correct = 0\n","        total = 0\n","        \n","        with torch.no_grad():\n","\n","            for images, targets in valloader:\n","                images, targets = images.to(device), targets.to(device)\n","\n","                output = model(images)\n","                preds = torch.argmax(output, dim=1)\n","\n","                correct += (preds == targets).sum().item()\n","                running_val_loss += criterion(output, targets).item()\n","                total += targets.size(0)\n","\n","            acc = correct / total\n","            val_acc.append(acc)\n","            val_losses.append(running_val_loss / len(valloader))\n","\n","        print(f\"Epoch: {i+1}, Train Loss: {train_losses[-1]:.3f}, Train Acc: {train_acc[-1]:.3f}, Val Loss: {val_losses[-1]:.3f}, Val Acc: {val_acc[-1]:.3f}\")\n","\n","    return train_losses, train_acc, val_losses, val_acc"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T14:04:54.820491Z","iopub.status.busy":"2023-01-18T14:04:54.819694Z","iopub.status.idle":"2023-01-18T15:36:07.365017Z","shell.execute_reply":"2023-01-18T15:36:07.363829Z","shell.execute_reply.started":"2023-01-18T14:04:54.820325Z"},"trusted":true},"outputs":[],"source":["epochs = 15\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), 0.01, momentum=0.9, weight_decay=1e-4)\n","scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0.0001)\n","\n","train_losses, train_acc, val_losses, val_acc = train(model, epochs, optimizer, criterion, scheduler, device, train_loader, val_loader)\n","print(\"  Finished Training  \")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T15:36:07.367486Z","iopub.status.busy":"2023-01-18T15:36:07.367138Z","iopub.status.idle":"2023-01-18T15:36:07.687089Z","shell.execute_reply":"2023-01-18T15:36:07.686194Z","shell.execute_reply.started":"2023-01-18T15:36:07.367456Z"},"trusted":true},"outputs":[],"source":["def plot_graphs(train_losses, train_acc, val_losses, val_acc):\n","    plt.figure(figsize=(15, 5))\n","    plt.subplot(1, 2, 1)\n","    plt.plot(train_losses, label=\"Train Loss\")\n","    plt.plot(val_losses, label=\"Test Loss\")\n","    plt.legend()\n","    plt.subplot(1, 2, 2)\n","    plt.plot(train_acc, label=\"Train Accuracy\")\n","    plt.plot(val_acc, label=\"Test Accuracy\")\n","    plt.legend()\n","    plt.show()\n","\n","plot_graphs(train_losses, train_acc, val_losses, val_acc)"]},{"cell_type":"markdown","metadata":{},"source":["## Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T15:36:07.688907Z","iopub.status.busy":"2023-01-18T15:36:07.688567Z","iopub.status.idle":"2023-01-18T15:36:22.417590Z","shell.execute_reply":"2023-01-18T15:36:22.416359Z","shell.execute_reply.started":"2023-01-18T15:36:07.688871Z"},"trusted":true},"outputs":[],"source":["def test(model, testloader, device):\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for images, targets in testloader:\n","            images, targets = images.to(device), targets.to(device)\n","            output = model(images)\n","            preds = torch.argmax(output, dim=1)\n","            correct += (preds == targets).sum().item()\n","            total += targets.size(0)\n","\n","    print(f\"Test Accuracy: {correct / total:.3f}\")\n","\n","test(model, test_loader, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T15:36:22.420335Z","iopub.status.busy":"2023-01-18T15:36:22.420001Z","iopub.status.idle":"2023-01-18T15:36:24.467695Z","shell.execute_reply":"2023-01-18T15:36:24.466796Z","shell.execute_reply.started":"2023-01-18T15:36:22.420305Z"},"trusted":true},"outputs":[],"source":["def show_test_batch(images, preds, targets, class_map):\n","    images = denormalize(images.cpu())\n","    images = images.numpy()\n","    preds = preds.cpu().numpy()\n","    targets = targets.cpu().numpy()\n","\n","    fig, ax = plt.subplots(4, 4, figsize=(15, 15))\n","    fig.tight_layout()\n","    for i in range(4):\n","        for j in range(4):\n","            ax[i, j].imshow(np.transpose(images[i*4+j], (1, 2, 0)))\n","            ax[i, j].set_title(f\"Actual: {class_map[targets[i*4+j]]}\\nPred: {class_map[preds[i*4+j]]}\")\n","            ax[i, j].axis(\"off\")\n","\n","images, targets = next(iter(test_loader))\n","images, targets = images.to(device), targets.to(device)\n","output = model(images)\n","preds = torch.argmax(output, dim=1)\n","\n","show_test_batch(images, preds, targets, bird_name_map)"]},{"cell_type":"markdown","metadata":{},"source":["## Saving the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T15:36:24.469754Z","iopub.status.busy":"2023-01-18T15:36:24.469158Z","iopub.status.idle":"2023-01-18T15:36:24.897085Z","shell.execute_reply":"2023-01-18T15:36:24.895758Z","shell.execute_reply.started":"2023-01-18T15:36:24.469716Z"},"trusted":true},"outputs":[],"source":["torch.save(model.state_dict(), 'birds_resnet34_model.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T15:36:24.899014Z","iopub.status.busy":"2023-01-18T15:36:24.898659Z","iopub.status.idle":"2023-01-18T15:36:24.906219Z","shell.execute_reply":"2023-01-18T15:36:24.905084Z","shell.execute_reply.started":"2023-01-18T15:36:24.898972Z"},"trusted":true},"outputs":[],"source":["from IPython.display import FileLink\n","FileLink('birds_resnet34_model.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T15:36:24.908483Z","iopub.status.busy":"2023-01-18T15:36:24.907726Z","iopub.status.idle":"2023-01-18T15:36:24.921472Z","shell.execute_reply":"2023-01-18T15:36:24.920521Z","shell.execute_reply.started":"2023-01-18T15:36:24.908444Z"},"trusted":true},"outputs":[],"source":["import json\n","\n","json.dump( bird_name_map, open( \"birds_name_mapping.json\", 'w' ) )\n","FileLink('birds_name_mapping.json')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
