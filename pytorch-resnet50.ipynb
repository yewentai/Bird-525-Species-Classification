{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-12T00:06:32.727616Z","iopub.status.busy":"2023-12-12T00:06:32.726815Z"},"id":"4Siqg0CJDbCI","trusted":true},"outputs":[],"source":["# install dependencies\n","%pip install numpy\n","%pip install matplotlib\n","%pip install torch torchvision\n","%pip install opencv-python\n","%pip install Pillow\n","%pip install imutils\n","%pip install super_gradients\n","%pip install albumentations \n","%pip install split-folders"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pam_bZ2aDYXF","trusted":true},"outputs":[],"source":["import math\n","import random\n","from typing import List,Tuple\n","import requests\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from pathlib import Path, PurePath\n","import pathlib\n","\n","import torch\n","from torch.utils.data import DataLoader\n","import torchvision\n","from torchvision import datasets\n","from torchvision import transforms\n","\n","from PIL import Image\n","\n","import albumentations as A\n","\n","from imutils import paths\n","\n","import textwrap\n","\n","from super_gradients.training import Trainer\n","from super_gradients.training import training_hyperparams\n","from super_gradients.training.metrics.classification_metrics import Accuracy, Top5\n","from super_gradients.training.utils.early_stopping import EarlyStop\n","from super_gradients.training import models\n","from super_gradients.training.utils.callbacks import Phase"]},{"cell_type":"markdown","metadata":{"id":"SbT5tUywqnFr"},"source":["# Number of classes\n","\n","Simple utility to fetch the number of classes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g0rFIshTqmsK","trusted":true},"outputs":[],"source":["def count_subdirectories(path: str) -> int:\n","    \"\"\"\n","    Counts the number of subdirectories in the given directory path.\n","    \"\"\"\n","    dir_path = Path(path)\n","    subdirectories = [f for f in dir_path.iterdir() if f.is_dir()]\n","    return len(subdirectories)\n","\n","# Example usage\n","parent_dir = \"./data/train\"\n","num_species = count_subdirectories(parent_dir)\n","print(f\"Number of subdirectories in {parent_dir}: {num_species}\")"]},{"cell_type":"markdown","metadata":{"id":"gPKlFKEJDfWF"},"source":["# Config \n","\n","This holds variables for the notebook.\n","\n","Define the model, training params, image type, number of classes, and  relevant directories in this class."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5bsqNIJ7DoO2","trusted":true},"outputs":[],"source":["class config:\n","    # specify the paths to datasets\n","    ROOT_DIR = Path('./data/')\n","    TRAIN_DIR = ROOT_DIR.joinpath('train')\n","    TEST_DIR = ROOT_DIR.joinpath('test')\n","    VAL_DIR = ROOT_DIR.joinpath('valid')\n","\n","    # set the input height and width\n","    INPUT_HEIGHT = 224\n","    INPUT_WIDTH = 224\n","\n","    IMAGENET_MEAN = [0.485, 0.456, 0.406]\n","    IMAGENET_STD = [0.229, 0.224, 0.225]\n","    \n","    IMAGE_TYPE = '.jpg'\n","    BATCH_SIZE = 128\n","    MODEL_NAME = 'resnet50'\n","    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    TRAINING_PARAMS = 'training_hyperparams/imagenet_resnet50_train_params'\n","    \n","    NUM_CLASSES = num_species\n","    \n","    CHECKPOINT_DIR = ROOT_DIR.joinpath('checkpoints')\n"]},{"cell_type":"markdown","metadata":{"id":"9o-LvyR0D1AK"},"source":["# Plot random images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lMarOo68FmDu","outputId":"51ee4508-fc67-401d-c903-25c69a603072","trusted":true},"outputs":[],"source":["train_image_path_list = list(sorted(paths.list_images(config.TRAIN_DIR)))\n","train_image_path_sample = random.sample(population=train_image_path_list, k=9)\n","\n","def examine_images(images:list):\n","    num_images = len(images)\n","    num_rows = int(math.ceil(num_images/3))\n","    num_cols = 3\n","    \n","    fig, axs = plt.subplots(num_rows, num_cols, figsize=(10, 10),tight_layout=True)\n","    axs = axs.ravel()\n","\n","    for i, image_path in enumerate(images[:num_images]):\n","        image = Image.open(image_path)\n","        label = PurePath(image_path).parent.name\n","        axs[i].imshow(image)\n","        axs[i].set_title(f\"Species: {label}\", fontsize=10)\n","        axs[i].axis('off')\n","    \n","    # save the sample images\n","    plt.savefig('./figures/sample_images.png', format='png', dpi=600)\n","\n","examine_images(train_image_path_sample)"]},{"cell_type":"markdown","metadata":{"id":"Ic8WXQILKc1y"},"source":["# Augmentations\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PzQIE96DKdKf","trusted":true},"outputs":[],"source":["# initialize our data augmentation functions\n","resize = transforms.Resize(size=(config.INPUT_HEIGHT,config.INPUT_WIDTH))\n","make_tensor = transforms.ToTensor()\n","normalize = transforms.Normalize(mean=config.IMAGENET_MEAN, std=config.IMAGENET_STD)\n","center_cropper = transforms.CenterCrop((config.INPUT_HEIGHT,config.INPUT_WIDTH))\n","random_horizontal_flip = transforms.RandomHorizontalFlip(p=0.5)\n","random_vertical_flip = transforms.RandomVerticalFlip(p=0.5)\n","random_rotation = transforms.RandomRotation(degrees=180)\n","random_crop = transforms.RandomCrop(size=(170,170))\n","random_erasing = transforms.RandomErasing()\n","augmix = transforms.AugMix(severity = 4, mixture_width=4, alpha=0.65)\n","\n","# initialize our training and validation set data augmentation pipeline\n","train_transforms = transforms.Compose([\n","  resize, \n","  augmix,\n","  center_cropper,\n","  random_crop,\n","  random_horizontal_flip,\n","  random_vertical_flip,\n","  random_rotation,\n","  make_tensor,\n","  normalize\n","])\n","\n","val_transforms = transforms.Compose([resize, make_tensor, normalize])"]},{"cell_type":"markdown","metadata":{"id":"foobRpfyKfeB"},"source":["# Show what one image looks like after augmentation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LFNBjz6JKfkA","trusted":true},"outputs":[],"source":["def apply_transform(img: Image, transform) -> np.ndarray:\n","    \"\"\"\n","    Applies a transform to a PIL Image and returns a numpy array of the transformed image.\n","\n","    Args:\n","        img (PIL.Image): The input image to transform.\n","        transform (torchvision.transforms.Compose): The transform to apply to the image.\n","\n","    Returns:\n","        np.ndarray: A numpy array representing the transformed image.\n","    \"\"\"\n","    # Apply the transform to the image\n","    if isinstance(transform, torchvision.transforms.Compose):\n","        # Apply PyTorch transform to image array\n","        transformed_image = train_transforms(img)\n","\n","    elif isinstance(transform, A.Compose):\n","        # Apply Albumentations transform to image array\n","        img_array = np.array(img)\n","        transformed_image = transform(image=img_array)[\"image\"]\n","\n","    # Convert the image tensor to a numpy array and transpose the axes to (height, width, channels)\n","    img_array = transformed_image.numpy().transpose((1, 2, 0))\n","\n","    # Clip the pixel values to the range [0, 1]\n","    img_array = np.clip(img_array, 0, 1)\n","\n","    return img_array\n","\n","\n","def visualize_transform(image: np.ndarray, original_image: np.ndarray = None) -> None:\n","    \"\"\"\n","    Visualize the transformed image.\n","\n","    Args:\n","        image (np.ndarray): A NumPy array representing the transformed image.\n","        original_image (np.ndarray, optional): A NumPy array representing the original image. Defaults to None.\n","    \"\"\"\n","    fontsize = 18\n","    \n","    if original_image is None:\n","        # Create a plot with 1 row and 2 columns.\n","        f, ax = plt.subplots(1, 2, figsize=(12, 12))\n","\n","        # Show the transformed image in the first column.\n","        ax[0].imshow(image)\n","    else:\n","        # Create a plot with 1 row and 2 columns.\n","        f, ax = plt.subplots(1, 2, figsize=(12, 12))\n","\n","        # Show the original image in the first column.\n","        ax[0].imshow(original_image)\n","        ax[0].set_title('Original image', fontsize=fontsize)\n","        \n","        # Show the transformed image in the second column.\n","        ax[1].imshow(image)\n","        ax[1].set_title('Transformed image', fontsize=fontsize)\n","        \n","img = Image.open(train_image_path_list[0])\n","img_array = apply_transform(img, train_transforms)\n","\n","visualize_transform(img_array, original_image=img)"]},{"cell_type":"markdown","metadata":{"id":"1gVOPxIqFrPY"},"source":["# Datasets and Dataloadrer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GWga8jM0GCcU","trusted":true},"outputs":[],"source":["def create_dataloaders(\n","    train_dir: str, \n","    val_dir: str,\n","    test_dir: str,\n","    train_transform: transforms.Compose,\n","    val_transform:  transforms.Compose,\n","    test_transform:  transforms.Compose,\n","    batch_size: int, \n","    num_workers: int=2\n","):\n","  \"\"\"Creates training and validation DataLoaders.\n","  Args:\n","    train_dir: Path to training data.\n","    val_dir: Path to validation data.\n","    transform: Transformation pipeline.\n","    batch_size: Number of samples per batch in each of the DataLoaders.\n","    num_workers: An integer for number of workers per DataLoader.\n","  Returns:\n","    A tuple of (train_dataloader, val_dataloader, class_names).\n","  \"\"\"\n","  # Use ImageFolder to create dataset\n","  train_data = datasets.ImageFolder(train_dir, transform=train_transform)\n","  val_data = datasets.ImageFolder(val_dir, transform=val_transform)\n","  test_data = datasets.ImageFolder(test_dir, transform=val_transform)  \n","\n","  print(f\"[INFO] training dataset contains {len(train_data)} samples...\")\n","  print(f\"[INFO] validation dataset contains {len(val_data)} samples...\")\n","  print(f\"[INFO] test dataset contains {len(test_data)} samples...\")\n","\n","  # Get class names\n","  class_names = train_data.classes\n","  print(f\"[INFO] dataset contains {len(class_names)} labels...\")\n","\n","  # Turn images into data loaders\n","  print(\"[INFO] creating training and validation set dataloaders...\")\n","  train_dataloader = DataLoader(\n","      train_data,\n","      batch_size=batch_size,\n","      shuffle=True,\n","      drop_last=True,\n","      num_workers=num_workers,\n","      pin_memory=True,\n","      persistent_workers=True\n","  )\n","  val_dataloader = DataLoader(\n","      val_data,\n","      batch_size=batch_size,\n","      shuffle=False,\n","      num_workers=num_workers,\n","      pin_memory=True,\n","      drop_last=False,\n","      persistent_workers=True\n","  )\n","\n","  test_dataloader = DataLoader(\n","      test_data,\n","      batch_size=batch_size,\n","      shuffle=False,\n","      num_workers=num_workers,\n","      pin_memory=True,\n","      drop_last=False,\n","      persistent_workers=True\n","  )\n","\n","  return train_dataloader, val_dataloader, test_dataloader, class_names"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_dataloader, valid_dataloader, test_dataloader, class_names = create_dataloaders(train_dir=config.TRAIN_DIR,\n","                                                                     val_dir=config.VAL_DIR,\n","                                                                     test_dir=config.TEST_DIR,\n","                                                                     train_transform=train_transforms,\n","                                                                     val_transform=val_transforms,\n","                                                                     test_transform=val_transforms,\n","                                                                     batch_size=config.BATCH_SIZE)\n","\n","NUM_CLASSES = len(class_names)"]},{"cell_type":"markdown","metadata":{"id":"GG6RomNeGCnc"},"source":["# Training Params"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yn5OpeiPGEsu","trusted":true},"outputs":[],"source":["training_params =  training_hyperparams.get(config.TRAINING_PARAMS)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["early_stop_acc = EarlyStop(Phase.VALIDATION_EPOCH_END, monitor=\"Accuracy\", mode=\"max\", patience=7, verbose=False)\n","early_stop_val_loss = EarlyStop(Phase.VALIDATION_EPOCH_END, monitor=\"LabelSmoothingCrossEntropyLoss\", mode=\"min\", patience=7, verbose=False)\n","\n","training_params[\"train_metrics_list\"] = [Accuracy(), Top5()]\n","training_params[\"valid_metrics_list\"] = [Accuracy(), Top5()]\n","training_params[\"phase_callbacks\"] = [early_stop_acc, early_stop_val_loss]\n","\n","# Set the silent mode to True to reduce clutter in the notebook, you can turn it on to see the full output\n","training_params[\"silent_mode\"] = True\n","\n","# Turn off the use of exponential moving average and zero weight decay on bias and batch norm\n","# training_params['ema'] = False\n","training_params['zero_weight_decay_on_bias_and_bn'] = False\n","training_params[\"optimizer\"] = 'Adam'\n","\n","training_params[\"criterion_params\"] = {'smooth_eps': 0.20}\n","training_params[\"max_epochs\"] = 250\n","training_params[\"initial_lr\"] = 0.0001"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["training_params"]},{"cell_type":"markdown","metadata":{"id":"Glmd2R98GZmd"},"source":["# Get model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rht8gYzaGFFn","trusted":true},"outputs":[],"source":["# get pretrained model from torchvision\n","model = models.get(config.MODEL_NAME, num_classes = config.NUM_CLASSES, pretrained_weights='imagenet')"]},{"cell_type":"markdown","metadata":{"id":"ZliLB58FGFYr"},"source":["# Instantiate trainer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uBGak3EPGHVA","trusted":true},"outputs":[],"source":["model_trainer = Trainer(experiment_name='ResNet50', ckpt_root_dir=config.CHECKPOINT_DIR)"]},{"cell_type":"markdown","metadata":{"id":"7zwKpRYiGcIK"},"source":["# Train model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VaavBbnFGcat","trusted":true},"outputs":[],"source":["model_trainer.train(model=model, \n","              training_params=training_params, \n","              train_loader=train_dataloader,\n","              valid_loader=valid_dataloader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l0kvMZwYGkdB","trusted":true},"outputs":[],"source":["import os\n","trained_model = models.get(config.MODEL_NAME,\n","                        num_classes=config.NUM_CLASSES,\n","                        checkpoint_path=os.path.join(model_trainer.checkpoints_dir_path, \"ckpt_best.pth\"))"]},{"cell_type":"markdown","metadata":{"id":"u-n1DyypGkjr"},"source":["# Evaluate on test set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z8H6camVGmY3","trusted":true},"outputs":[],"source":["model_trainer.test(model=trained_model,\n","            test_loader=test_dataloader,\n","            test_metrics_list=['Accuracy', 'Top5'])"]},{"cell_type":"markdown","metadata":{"id":"6aNH0FiQGmvZ"},"source":["# Plot predictions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def predict(image_path: str, \n","            class_names: List[str] = class_names,\n","            model: torch.nn.Module = trained_model,\n","            image_size: Tuple[int, int] = (config.INPUT_HEIGHT, config.INPUT_WIDTH),\n","            transform: torchvision.transforms = None,\n","            device: torch.device=config.DEVICE):\n","\n","    if isinstance(image_path, pathlib.PosixPath):\n","        img = Image.open(image_path)\n","    else: \n","        img = Image.open(requests.get(image_path, stream=True).raw)\n","\n","    # create transformation for image (if one doesn't exist)\n","    if transform is None:\n","        transform = transforms.Compose([\n","            transforms.Resize(image_size),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=config.IMAGENET_MEAN,\n","                                 std=config.IMAGENET_STD),\n","        ])\n","    transformed_image = transform(img)\n","\n","    # make sure the model is on the target device\n","    model.to(device)\n","\n","    # turn on model evaluation mode and inference mode\n","    model.eval()\n","    with torch.inference_mode():\n","        # add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])\n","        transformed_image = transformed_image.unsqueeze(dim=0)\n","\n","        # make a prediction on image with an extra dimension and send it to the target device\n","        target_image_pred = model(transformed_image.to(device))\n","\n","    # convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n","    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n","\n","    # convert prediction probabilities -> prediction labels\n","    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n","\n","    # actual label\n","    ground_truth = PurePath(image_path).parent.name\n","\n","    # predicted label\n","    prediction = class_names[target_image_pred_label]\n","\n","    # predicted probability\n","    prediction_prob = target_image_pred_probs[0][target_image_pred_label].item()\n","\n","    return prediction, prediction_prob, ground_truth\n","    \n","# predict on a single image\n","test_image_path_list = list(Path(config.TEST_DIR).glob(\"*/*.jpg\")) # get list all image paths from test data \n","test_image_path_sample = random.sample(population=test_image_path_list, # go through all of the test image paths\n","                                        k=1) # sample 1 image path\n","\n","# Make predictions on and plot the images\n","for i, image_path in enumerate(test_image_path_sample):\n","    prediction, prediction_prob, ground_truth = predict(image_path=image_path, \n","                                                        class_names=class_names,\n","                                                        model=trained_model,\n","                                                        image_size=(config.INPUT_HEIGHT, config.INPUT_WIDTH),\n","                                                        transform=val_transforms,\n","                                                        device=config.DEVICE)\n","    # load the image\n","    image = Image.open(image_path)\n","\n","    # plot the image\n","    plt.imshow(image)\n","    plt.title(f\"Prediction: {prediction} ({prediction_prob*100:.2f}%) \\n Ground Truth: {ground_truth}\", fontsize=14)\n","    plt.axis('off')\n","    plt.savefig('./figures/single_image_prediction.png', format='png', dpi=600)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot confusion matrix\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","import pandas as pd\n","\n","def plot_confusion_matrix(y_true, y_pred, class_names):\n","    \"\"\"\n","    Plots a confusion matrix using seaborn's heatmap() function.\n","    \"\"\"\n","    cm = confusion_matrix(y_true, y_pred, normalize='true')\n","    cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n","\n","    fig, ax = plt.subplots()\n","    sns.heatmap(cm_df, annot=True, cmap='Blues', ax=ax, fmt='.2f')\n","    ax.set_title('Confusion Matrix')\n","    ax.set_ylabel('True Label')\n","    ax.set_xlabel('Predicted Label')\n","    plt.savefig('./figures/confusion_matrix.png', format='png')\n","    plt.show()\n","\n","# predict on all images in test set\n","test_image_path_list = list(Path(config.TEST_DIR).glob(\"*/*.jpg\")) # get list all image paths from test data \n","# Make predictions on and plot the images\n","y_true = []\n","y_pred = []\n","for i, image_path in enumerate(test_image_path_list):\n","    prediction, prediction_prob, ground_truth = predict(image_path=image_path, \n","                                                        class_names=class_names,\n","                                                        model=trained_model,\n","                                                        image_size=(config.INPUT_HEIGHT, config.INPUT_WIDTH),\n","                                                        transform=val_transforms,\n","                                                        device=config.DEVICE)\n","    y_true.append(ground_truth)\n","    y_pred.append(prediction)\n","\n","plot_confusion_matrix(y_true, y_pred, class_names)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P0pwt5j_G5a5","trusted":true},"outputs":[],"source":["def pred_and_plot_image(image_path: str, \n","                        subplot: Tuple[int, int, int],  # subplot tuple for `subplot()` function\n","                        class_names: List[str] = class_names,\n","                        model: torch.nn.Module = trained_model,\n","                        image_size: Tuple[int, int] = (config.INPUT_HEIGHT, config.INPUT_WIDTH),\n","                        transform: torchvision.transforms = None,\n","                        device: torch.device=config.DEVICE):\n","\n","    if isinstance(image_path, pathlib.PosixPath):\n","        img = Image.open(image_path)\n","    else: \n","        img = Image.open(requests.get(image_path, stream=True).raw)\n","\n","    # create transformation for image (if one doesn't exist)\n","    if transform is None:\n","        transform = transforms.Compose([\n","            transforms.Resize(image_size),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=config.IMAGENET_MEAN,\n","                                 std=config.IMAGENET_STD),\n","        ])\n","    transformed_image = transform(img)\n","\n","    # make sure the model is on the target device\n","    model.to(device)\n","\n","    # turn on model evaluation mode and inference mode\n","    model.eval()\n","    with torch.inference_mode():\n","        # add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])\n","        transformed_image = transformed_image.unsqueeze(dim=0)\n","\n","        # make a prediction on image with an extra dimension and send it to the target device\n","        target_image_pred = model(transformed_image.to(device))\n","\n","    # convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n","    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n","\n","    # convert prediction probabilities -> prediction labels\n","    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n","\n","    # actual label\n","    ground_truth = PurePath(image_path).parent.name\n","\n","    # plot image with predicted label and probability \n","    plt.subplot(*subplot)\n","    plt.imshow(img)\n","    if isinstance(image_path, pathlib.PosixPath):\n","        title = f\"Ground Truth: {ground_truth} | Pred: {class_names[target_image_pred_label]} | Prob: {target_image_pred_probs.max():.3f}\"\n","    else:\n","        title = f\"Pred: {class_names[target_image_pred_label]} | Prob: {target_image_pred_probs.max():.3f}\"\n","    plt.title(\"\\n\".join(textwrap.wrap(title, width=25)),fontsize=10)  # wrap text using textwrap.wrap() function\n","    plt.axis(False)\n","    \n","\n","def plot_random_test_images(model):\n","    num_images_to_plot = 9\n","    test_image_path_list = list(Path(config.TEST_DIR).glob(\"*/*.jpg\")) # get list all image paths from test data \n","    test_image_path_sample = random.sample(population=test_image_path_list, # go through all of the test image paths\n","                                           k=num_images_to_plot) # randomly select 'k' image paths to pred and plot\n","\n","    # set up subplots\n","    num_rows = int(np.ceil(num_images_to_plot / 3))\n","    fig, ax = plt.subplots(num_rows, 3, figsize=(10, num_rows * 3))\n","    ax = ax.flatten()\n","\n","    # Make predictions on and plot the images\n","    for i, image_path in enumerate(test_image_path_sample):\n","        pred_and_plot_image(model=model, \n","                            image_path=image_path,\n","                            class_names=class_names,\n","                            subplot=(num_rows, 3, i+1),  # subplot tuple for `subplot()` function\n","                            image_size=(config.INPUT_HEIGHT, config.INPUT_WIDTH))\n","\n","    # adjust spacing between subplots\n","    plt.subplots_adjust(wspace=1)\n","    plt.show()\n","    # save the predictions plot\n","    plt.savefig('./figures/predictions.png', format='png', dpi=300)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plot_random_test_images(trained_model)"]},{"cell_type":"markdown","metadata":{"id":"hGl1WHkXG0I4"},"source":["# Predict on images from internet"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#Sri Lankan blue magpie\n","image_url = 'https://cdn.download.ams.birds.cornell.edu/api/v1/asset/252002651/1800'\n","pred_and_plot_image(image_path= image_url, subplot=(1, 1, 1))"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":534640,"sourceId":5468571,"sourceType":"datasetVersion"}],"dockerImageVersionId":30396,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":4}
