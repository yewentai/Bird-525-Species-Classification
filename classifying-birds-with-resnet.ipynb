{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-12T00:06:32.727616Z","iopub.status.busy":"2023-12-12T00:06:32.726815Z"},"id":"4Siqg0CJDbCI","trusted":true},"outputs":[],"source":["# !pip install imutils\n","# !pip install super_gradients==3.0.7\n","# !pip install albumentations \n","# !pip install split-folders[full]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pam_bZ2aDYXF","trusted":true},"outputs":[],"source":["import math\n","import random\n","from typing import Dict, List,Tuple\n","import requests\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import glob\n","from pathlib import Path, PurePath\n","import pathlib\n","import pandas as pd\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision\n","from torchvision import datasets\n","from torchvision import transforms\n","\n","from PIL import Image\n","\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","from sklearn.model_selection import train_test_split\n","\n","from imutils import paths\n","\n","import splitfolders\n","import textwrap\n","\n","import super_gradients\n","from super_gradients.common.object_names import Models\n","from super_gradients.training import Trainer\n","from super_gradients.training import training_hyperparams\n","from super_gradients.training.metrics.classification_metrics import Accuracy, Top5\n","from super_gradients.training.utils.early_stopping import EarlyStop\n","from super_gradients.training import models\n","from super_gradients.training.utils.callbacks import Phase"]},{"cell_type":"markdown","metadata":{"id":"SbT5tUywqnFr"},"source":["# Number of classes\n","\n","Simple utility to fetch the number of classes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g0rFIshTqmsK","trusted":true},"outputs":[],"source":["from pathlib import Path\n","\n","def count_subdirectories(path: str) -> int:\n","    \"\"\"\n","    Counts the number of subdirectories in the given directory path.\n","    \"\"\"\n","    dir_path = Path(path)\n","    subdirectories = [f for f in dir_path.iterdir() if f.is_dir()]\n","    return len(subdirectories)\n","\n","# Example usage\n","parent_dir = \"../input/100-bird-species/train\"\n","num_subdirectories = count_subdirectories(parent_dir)\n","print(f\"Number of subdirectories in {parent_dir}: {num_subdirectories}\")"]},{"cell_type":"markdown","metadata":{"id":"gPKlFKEJDfWF"},"source":["# Config \n","\n","This holds variables for the notebook.\n","\n","You will define the model, training params, image type, number of classes, and \n","relevant directories in this class."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5bsqNIJ7DoO2","trusted":true},"outputs":[],"source":["class config:\n","    # specify the paths to datasets\n","    ROOT_DIR = Path('../input/100-bird-species')\n","    TRAIN_DIR = ROOT_DIR.joinpath('train')\n","    TEST_DIR = ROOT_DIR.joinpath('test')\n","    VAL_DIR = ROOT_DIR.joinpath('valid')\n","\n","    # set the input height and width\n","    INPUT_HEIGHT = 224\n","    INPUT_WIDTH = 224\n","\n","    # set the input heig/ht and width\n","    IMAGENET_MEAN = [0.485, 0.456, 0.406]\n","    IMAGENET_STD = [0.229, 0.224, 0.225]\n","    \n","    IMAGE_TYPE = '.jpg'\n","    BATCH_SIZE = 128\n","    MODEL_NAME = 'resnet50'\n","    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    TRAINING_PARAMS = 'training_hyperparams/imagenet_resnet50_train_params'\n","    \n","    NUM_CLASSES = num_subdirectories\n","#     IMAGE_TYPE = \n","    \n","    CHECKPOINT_DIR = 'checkpoints'\n"]},{"cell_type":"markdown","metadata":{"id":"9o-LvyR0D1AK"},"source":["# Plot random images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lMarOo68FmDu","outputId":"51ee4508-fc67-401d-c903-25c69a603072","trusted":true},"outputs":[],"source":["train_image_path_list = list(sorted(paths.list_images(config.TRAIN_DIR)))\n","train_image_path_sample = random.sample(population=train_image_path_list, k=20)\n","\n","def examine_images(images:list):\n","    num_images = len(images)\n","    num_rows = int(math.ceil(num_images/5))\n","    num_cols = 5\n","    \n","    fig, axs = plt.subplots(num_rows, num_cols, figsize=(30, 30),tight_layout=True)\n","    axs = axs.ravel()\n","\n","    for i, image_path in enumerate(images[:num_images]):\n","        image = Image.open(image_path)\n","        label = PurePath(image_path).parent.name\n","        axs[i].imshow(image)\n","        axs[i].set_title(f\"Bird: {label}\", fontsize=25)\n","        axs[i].axis('off')\n","    plt.show()\n","\n","examine_images(train_image_path_sample)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!head ../input/100-bird-species/birds.csv"]},{"cell_type":"markdown","metadata":{"id":"Ic8WXQILKc1y"},"source":["# Augmentations\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PzQIE96DKdKf","trusted":true},"outputs":[],"source":["# initialize our data augmentation functions\n","resize = transforms.Resize(size=(config.INPUT_HEIGHT,config.INPUT_WIDTH))\n","make_tensor = transforms.ToTensor()\n","normalize = transforms.Normalize(mean=config.IMAGENET_MEAN, std=config.IMAGENET_STD)\n","center_cropper = transforms.CenterCrop((config.INPUT_HEIGHT,config.INPUT_WIDTH))\n","random_horizontal_flip = transforms.RandomHorizontalFlip(p=0.5)\n","random_vertical_flip = transforms.RandomVerticalFlip(p=0.5)\n","random_rotation = transforms.RandomRotation(degrees=180)\n","random_crop = transforms.RandomCrop(size=(170,170))\n","random_erasing = transforms.RandomErasing()\n","augmix = transforms.AugMix(severity = 4, mixture_width=4, alpha=0.65)\n","\n","# randomly_choose_one \n","\n","# initialize our training and validation set data augmentation pipeline\n","train_transforms = transforms.Compose([\n","  resize, \n","  augmix,\n","  center_cropper,\n","  random_crop,\n","  random_horizontal_flip,\n","  random_vertical_flip,\n","  random_rotation,\n","  make_tensor,\n","  normalize\n","])\n","\n","val_transforms = transforms.Compose([resize, make_tensor, normalize])"]},{"cell_type":"markdown","metadata":{"id":"foobRpfyKfeB"},"source":["# Show what one image looks like after augmentation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LFNBjz6JKfkA","trusted":true},"outputs":[],"source":["def apply_transform(img: Image, transform) -> np.ndarray:\n","    \"\"\"\n","    Applies a transform to a PIL Image and returns a numpy array of the transformed image.\n","\n","    Args:\n","        img (PIL.Image): The input image to transform.\n","        transform (torchvision.transforms.Compose): The transform to apply to the image.\n","\n","    Returns:\n","        np.ndarray: A numpy array representing the transformed image.\n","    \"\"\"\n","    # Apply the transform to the image\n","    if isinstance(transform, torchvision.transforms.Compose):\n","        # Apply PyTorch transform to image array\n","        transformed_image = train_transforms(img)\n","\n","    elif isinstance(transform, A.Compose):\n","        # Apply Albumentations transform to image array\n","        img_array = np.array(img)\n","        transformed_image = transform(image=img_array)[\"image\"]\n","\n","    # Convert the image tensor to a numpy array and transpose the axes to (height, width, channels)\n","    img_array = transformed_image.numpy().transpose((1, 2, 0))\n","\n","    # Clip the pixel values to the range [0, 1]\n","    img_array = np.clip(img_array, 0, 1)\n","\n","    return img_array\n","\n","\n","def visualize_transform(image: np.ndarray, original_image: np.ndarray = None) -> None:\n","    \"\"\"\n","    Visualize the transformed image.\n","\n","    Args:\n","        image (np.ndarray): A NumPy array representing the transformed image.\n","        original_image (np.ndarray, optional): A NumPy array representing the original image. Defaults to None.\n","    \"\"\"\n","    fontsize = 18\n","    \n","    if original_image is None:\n","        # Create a plot with 1 row and 2 columns.\n","        f, ax = plt.subplots(1, 2, figsize=(12, 12))\n","\n","        # Show the transformed image in the first column.\n","        ax[0].imshow(image)\n","    else:\n","        # Create a plot with 1 row and 2 columns.\n","        f, ax = plt.subplots(1, 2, figsize=(12, 12))\n","\n","        # Show the original image in the first column.\n","        ax[0].imshow(original_image)\n","        ax[0].set_title('Original image', fontsize=fontsize)\n","        \n","        # Show the transformed image in the second column.\n","        ax[1].imshow(image)\n","        ax[1].set_title('Transformed image', fontsize=fontsize)\n","        \n","img = Image.open(train_image_path_list[0])\n","img_array = apply_transform(img, train_transforms)\n","\n","visualize_transform(img_array, original_image=img)"]},{"cell_type":"markdown","metadata":{"id":"1gVOPxIqFrPY"},"source":["# Datasets and Dataloadrer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B1aikxZRGCKr","trusted":true},"outputs":[],"source":["def create_dataloaders(train_dir: str, val_dir: str, test_dir: str,\n","                       train_transform: transforms.Compose, val_transform: transforms.Compose,\n","                       test_transform: transforms.Compose, batch_size: int, num_workers: int = 2):\n","    \"\"\"Creates training and validation DataLoaders.\n","\n","    Args:\n","        train_dir (str): Path to training data.\n","        val_dir (str): Path to validation data.\n","        test_dir (str): Path to test data.\n","        train_transform (torchvision.transforms.Compose): Transformation pipeline for training data.\n","        val_transform (torchvision.transforms.Compose): Transformation pipeline for validation data.\n","        test_transform (torchvision.transforms.Compose): Transformation pipeline for test data.\n","        batch_size (int): Number of samples per batch in each of the DataLoaders.\n","        num_workers (int): Number of workers per DataLoader.\n","\n","    Returns:\n","        Tuple of (train_dataloader, val_dataloader, test_dataloader, class_names).\n","    \"\"\"\n","    # Use ImageFolder to create dataset\n","    train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)\n","    val_dataset = datasets.ImageFolder(val_dir, transform=val_transform)\n","    test_dataset = datasets.ImageFolder(test_dir, transform=test_transform)\n","\n","    print(f\"[INFO] Training dataset contains {len(train_dataset)} samples.\")\n","    print(f\"[INFO] Validation dataset contains {len(val_dataset)} samples.\")\n","    print(f\"[INFO] Test dataset contains {len(test_dataset)} samples.\")\n","\n","    # Get class names\n","    class_names = train_dataset.classes\n","    print(f\"[INFO] Dataset contains {len(class_names)} labels.\")\n","    \n","    print(\"[INFO] creating training and validation set dataloaders...\")\n","    train_dataloader = DataLoader(\n","      train_data,\n","      batch_size=batch_size,\n","      shuffle=True,\n","      drop_last=True,\n","      num_workers=num_workers,\n","      pin_memory=True,\n","      persistent_workers=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GWga8jM0GCcU","trusted":true},"outputs":[],"source":["def create_dataloaders(\n","    train_dir: str, \n","    val_dir: str,\n","    test_dir: str,\n","    train_transform: transforms.Compose,\n","    val_transform:  transforms.Compose,\n","    test_transform:  transforms.Compose,\n","    batch_size: int, \n","    num_workers: int=2\n","):\n","  \"\"\"Creates training and validation DataLoaders.\n","  Args:\n","    train_dir: Path to training data.\n","    val_dir: Path to validation data.\n","    transform: Transformation pipeline.\n","    batch_size: Number of samples per batch in each of the DataLoaders.\n","    num_workers: An integer for number of workers per DataLoader.\n","  Returns:\n","    A tuple of (train_dataloader, val_dataloader, class_names).\n","  \"\"\"\n","  # Use ImageFolder to create dataset\n","  train_data = datasets.ImageFolder(train_dir, transform=train_transform)\n","  val_data = datasets.ImageFolder(val_dir, transform=val_transform)\n","  test_data = datasets.ImageFolder(test_dir, transform=val_transform)  \n","\n","  print(f\"[INFO] training dataset contains {len(train_data)} samples...\")\n","  print(f\"[INFO] validation dataset contains {len(val_data)} samples...\")\n","  print(f\"[INFO] test dataset contains {len(test_data)} samples...\")\n","\n","  # Get class names\n","  class_names = train_data.classes\n","  print(f\"[INFO] dataset contains {len(class_names)} labels...\")\n","\n","  # Turn images into data loaders\n","  print(\"[INFO] creating training and validation set dataloaders...\")\n","  train_dataloader = DataLoader(\n","      train_data,\n","      batch_size=batch_size,\n","      shuffle=True,\n","      drop_last=True,\n","      num_workers=num_workers,\n","      pin_memory=True,\n","      persistent_workers=True\n","  )\n","  val_dataloader = DataLoader(\n","      val_data,\n","      batch_size=batch_size,\n","      shuffle=False,\n","      num_workers=num_workers,\n","      pin_memory=True,\n","      drop_last=False,\n","      persistent_workers=True\n","  )\n","\n","  test_dataloader = DataLoader(\n","      test_data,\n","      batch_size=batch_size,\n","      shuffle=False,\n","      num_workers=num_workers,\n","      pin_memory=True,\n","      drop_last=False,\n","      persistent_workers=True\n","  )\n","\n","  return train_dataloader, val_dataloader, test_dataloader, class_names"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_dataloader, valid_dataloader, test_dataloader, class_names = create_dataloaders(train_dir=config.TRAIN_DIR,\n","                                                                     val_dir=config.VAL_DIR,\n","                                                                     test_dir=config.TEST_DIR,\n","                                                                     train_transform=train_transforms,\n","                                                                     val_transform=val_transforms,\n","                                                                     test_transform=val_transforms,\n","                                                                     batch_size=config.BATCH_SIZE)\n","\n","NUM_CLASSES = len(class_names)"]},{"cell_type":"markdown","metadata":{"id":"GG6RomNeGCnc"},"source":["# Training Params"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yn5OpeiPGEsu","trusted":true},"outputs":[],"source":["training_params =  training_hyperparams.get(config.TRAINING_PARAMS)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["training_params"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# To reduce clutter in the notebook I've turned the verbosity off, you can turn it on to see the full output\n","early_stop_acc = EarlyStop(Phase.VALIDATION_EPOCH_END, monitor=\"Accuracy\", mode=\"max\", patience=7, verbose=False)\n","early_stop_val_loss = EarlyStop(Phase.VALIDATION_EPOCH_END, monitor=\"LabelSmoothingCrossEntropyLoss\", mode=\"min\", patience=7, verbose=False)\n","\n","training_params[\"train_metrics_list\"] = [Accuracy(), Top5()]\n","training_params[\"valid_metrics_list\"] = [Accuracy(), Top5()]\n","training_params[\"phase_callbacks\"] = [early_stop_acc, early_stop_val_loss]\n","\n","# Set the silent mode to True to reduce clutter in the notebook, you can turn it on to see the full output\n","training_params[\"silent_mode\"] = True\n","# We'll turn off the use of exponential moving average and zero weight decay on bias and batch norm\n","# training_params['ema'] = False\n","training_params['zero_weight_decay_on_bias_and_bn'] = False\n","training_params[\"optimizer\"] = 'Adam'\n","\n","training_params[\"criterion_params\"] = {'smooth_eps': 0.20}\n","training_params[\"max_epochs\"] = 250\n","training_params[\"initial_lr\"] = 0.0001"]},{"cell_type":"markdown","metadata":{"id":"Glmd2R98GZmd"},"source":["# Get model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rht8gYzaGFFn","trusted":true},"outputs":[],"source":["model = models.get(config.MODEL_NAME, num_classes = config.NUM_CLASSES, pretrained_weights='imagenet')"]},{"cell_type":"markdown","metadata":{"id":"ZliLB58FGFYr"},"source":["# Instantiate trainer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uBGak3EPGHVA","trusted":true},"outputs":[],"source":["full_model_trainer = Trainer(experiment_name='0_Baseline_Experiment', ckpt_root_dir=config.CHECKPOINT_DIR)"]},{"cell_type":"markdown","metadata":{"id":"7zwKpRYiGcIK"},"source":["# Train model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VaavBbnFGcat","trusted":true},"outputs":[],"source":["full_model_trainer.train(model=model, \n","              training_params=training_params, \n","              train_loader=train_dataloader,\n","              valid_loader=valid_dataloader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l0kvMZwYGkdB","trusted":true},"outputs":[],"source":["import os\n","best_full_model = models.get(config.MODEL_NAME,\n","                        num_classes=config.NUM_CLASSES,\n","                        checkpoint_path=os.path.join(full_model_trainer.checkpoints_dir_path, \"average_model.pth\"))"]},{"cell_type":"markdown","metadata":{"id":"u-n1DyypGkjr"},"source":["# Evaluate on test set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z8H6camVGmY3","trusted":true},"outputs":[],"source":["full_model_trainer.test(model=best_full_model,\n","            test_loader=test_dataloader,\n","            test_metrics_list=['Accuracy', 'Top5'])"]},{"cell_type":"markdown","metadata":{},"source":["# Looks like we're able to achieve 99.16% accuracy on the test set!"]},{"cell_type":"markdown","metadata":{"id":"6aNH0FiQGmvZ"},"source":["# Plot predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P0pwt5j_G5a5","trusted":true},"outputs":[],"source":["def pred_and_plot_image(image_path: str, \n","                        subplot: Tuple[int, int, int],  # subplot tuple for `subplot()` function\n","                        class_names: List[str] = class_names,\n","                        model: torch.nn.Module = best_full_model,\n","                        image_size: Tuple[int, int] = (config.INPUT_HEIGHT, config.INPUT_WIDTH),\n","                        transform: torchvision.transforms = None,\n","                        device: torch.device=config.DEVICE):\n","\n","    if isinstance(image_path, pathlib.PosixPath):\n","        img = Image.open(image_path)\n","    else: \n","        img = Image.open(requests.get(image_path, stream=True).raw)\n","\n","    # create transformation for image (if one doesn't exist)\n","    if transform is None:\n","        transform = transforms.Compose([\n","            transforms.Resize(image_size),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=config.IMAGENET_MEAN,\n","                                 std=config.IMAGENET_STD),\n","        ])\n","    transformed_image = transform(img)\n","\n","    # make sure the model is on the target device\n","    model.to(device)\n","\n","    # turn on model evaluation mode and inference mode\n","    model.eval()\n","    with torch.inference_mode():\n","        # add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])\n","        transformed_image = transformed_image.unsqueeze(dim=0)\n","\n","        # make a prediction on image with an extra dimension and send it to the target device\n","        target_image_pred = model(transformed_image.to(device))\n","\n","    # convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n","    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n","\n","    # convert prediction probabilities -> prediction labels\n","    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n","\n","    # actual label\n","    ground_truth = PurePath(image_path).parent.name\n","\n","    # plot image with predicted label and probability \n","    plt.subplot(*subplot)\n","    plt.imshow(img)\n","    if isinstance(image_path, pathlib.PosixPath):\n","        title = f\"Ground Truth: {ground_truth} | Pred: {class_names[target_image_pred_label]} | Prob: {target_image_pred_probs.max():.3f}\"\n","    else:\n","        title = f\"Pred: {class_names[target_image_pred_label]} | Prob: {target_image_pred_probs.max():.3f}\"\n","    plt.title(\"\\n\".join(textwrap.wrap(title, width=20)))  # wrap text using textwrap.wrap() function\n","    plt.axis(False)\n","    \n","\n","def plot_random_test_images(model):\n","    num_images_to_plot = 30\n","    test_image_path_list = list(Path(config.TEST_DIR).glob(\"*/*.jpg\")) # get list all image paths from test data \n","    test_image_path_sample = random.sample(population=test_image_path_list, # go through all of the test image paths\n","                                           k=num_images_to_plot) # randomly select 'k' image paths to pred and plot\n","\n","    # set up subplots\n","    num_rows = int(np.ceil(num_images_to_plot / 5))\n","    fig, ax = plt.subplots(num_rows, 5, figsize=(15, num_rows * 3))\n","    ax = ax.flatten()\n","\n","    # Make predictions on and plot the images\n","    for i, image_path in enumerate(test_image_path_sample):\n","        pred_and_plot_image(model=model, \n","                            image_path=image_path,\n","                            class_names=class_names,\n","                            subplot=(num_rows, 5, i+1),  # subplot tuple for `subplot()` function\n","                            image_size=(config.INPUT_HEIGHT, config.INPUT_WIDTH))\n","\n","    # adjust spacing between subplots\n","    plt.subplots_adjust(wspace=1)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"hGl1WHkXG0I4"},"source":["# Predict on images from internet"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plot_random_test_images(best_full_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OC54y8rJG0AB","trusted":true},"outputs":[],"source":["#Baltimore oriole\n","image_url = 'https://www.allaboutbirds.org/guide/assets/og/75258971-1200px.jpg'\n","pred_and_plot_image(image_path= image_url, subplot=(1, 1, 1))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#Sri Lankan blue magpie\n","image_url = 'https://cdn.download.ams.birds.cornell.edu/api/v1/asset/252002651/1800'\n","pred_and_plot_image(image_path= image_url, subplot=(1, 1, 1))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#BLACK THROATED BUSHTIT\n","image_url = 'http://aabird.weebly.com/uploads/1/2/0/6/12063687/339097369.jpg'\n","pred_and_plot_image(image_path= image_url, subplot=(1, 1, 1))"]},{"cell_type":"markdown","metadata":{},"source":["# Your homework\n","\n","Copy/fork this notebook and try some different architectures.\n","\n","If you have a question you can leave a comment on this notebook, or visit the community and post it in the [Q&A section](https://www.deeplearningdaily.community/c/qanda/8).\n","\n","## Use a different pretrained model\n","\n","You can change the model you use. Take a look at the [SG model zoo](https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/training/Computer_Vision_Models_Pretrained_Checkpoints.md)\n","\n","For example, if you wanted to use RegNet you would do the following:\n","\n","```\n","resnet_imagenet_model = models.get(model_name='regnetY800', num_classes=NUM_CLASSES, pretrained_weights='imagenet)\n","resnet_params =  training_hyperparams.get('training_hyperparams/imagenet_regnetY_train_params')\n","```\n","\n","Note you can also pass 'model_name=regnetY200', 'model_name=regnetY400', 'model_name=regnetY600' to try a variety of the architecture\n","\n","For ResNet50, you would do:\n","\n","```\n","resnet_imagenet_model = models.get(model_name='resnet50', num_classes=NUM_CLASSES, pretrained_weights='imagenet)\n","resnet_params =  training_hyperparams.get('training_hyperparams/imagenet_resnet50_train_params')\n","```\n","\n","Note you can also pass 'model_name=resnet18' or 'model_name=resnet34' to try a variety of the architecture\n","\n","For MobileNetV2, you would do:\n","\n","```\n","mobilenet_imagenet_model = models.get(model_name='mobilenet_v2', num_classes=NUM_CLASSES, pretrained_weights='imagenet)\n","resnet_params =  training_hyperparams.get('training_hyperparams/imagenet_mobilenetv2_train_params')\n","```\n","\n","For MobileNetV3, you would do:\n","\n","```\n","mobilenet_imagenet_model = models.get(model_name='mobilenet_v3_large', num_classes=NUM_CLASSES, pretrained_weights='imagenet)\n","resnet_params =  training_hyperparams.get('training_hyperparams/imagenet_mobilenetv3_train_params')\n","```\n","\n","Note you can also pass 'model_name=mobilenet_v3_small' to try a variety of the architecture\n","\n","\n","For ViT, you would do:\n","\n","\n","```\n","vit_imagenet_model = models.get(model_name='vit_base', num_classes=NUM_CLASSES, pretrained_weights='imagenet')\n","vit_params =  training_hyperparams.get(\"training_hyperparams/imagenet_vit_train_params\")\n","```\n","\n","Note you can also pass 'model_name=vit_large' to try a variety of the architecture\n","\n","\n","I encourage you play around with different optimizers, all you have to do is change the value of `training_params[\"optimizer\"]`. You can use one of ['Adam','SGD','RMSProp'] out of the box. You can play around with the optimizer params as well.\n","\n","In general, play and tweak around the training recipies...\n","\n","## Training recipes\n","\n","SuperGradients has a number of [training recipes](https://github.com/Deci-AI/super-gradients/tree/master/src/super_gradients/recipes) you can use. [See here](https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/recipes/training_hyperparams/default_train_params.yaml) for more information about the training params.\n","\n","If you're using Weights and Biases to track your experiments, you would do the following\n","\n","```\n","sg_logger: wandb_sg_logger\n","sg_logger_params:\n","project_name: <YOUR PROJECT NAME>\n","entity: algo\n","api_server: https://wandb.research.deci.ai\n","save_checkpoints_remote: True\n","save_tensorboard_remote: True\n","save_logs_remote: True\n","```"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":534640,"sourceId":5468571,"sourceType":"datasetVersion"}],"dockerImageVersionId":30396,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":4}
